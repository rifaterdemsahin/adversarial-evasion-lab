# Module 1: Evasion Attacks

This module covers fundamental adversarial evasion attacks that can fool machine learning models.

## ğŸ“š Contents

### M1_V1_FGSM.ipynb
Introduction to the **Fast Gradient Sign Method (FGSM)**, one of the most fundamental adversarial attack techniques. This notebook covers:
- Understanding FGSM attack mechanism
- Implementation from scratch
- Testing on MNIST dataset
- Visualizing adversarial perturbations
- Analyzing attack success rates

## ğŸ¯ Learning Objectives

After completing this module, you will:
- Understand how gradient-based attacks work
- Be able to implement FGSM from scratch
- Know how to evaluate attack effectiveness
- Understand the trade-off between attack strength and perceptibility

## ğŸš€ Getting Started

1. Make sure you have installed all dependencies from the root `requirements.txt`
2. Launch Jupyter Notebook from the root directory
3. Open `M1_V1_FGSM.ipynb` and follow along

## ğŸ“ Images Directory

The `images/` directory is available for storing:
- Generated adversarial examples
- Visualization plots
- Comparison figures
- Any other static assets used in the notebooks

## ğŸ’¡ Tips

- Run cells sequentially to avoid errors
- Experiment with different epsilon values to see the impact
- Try the attack on different models and datasets
- Save interesting adversarial examples in the images folder

## ğŸ“– Additional Resources

- [Explaining and Harnessing Adversarial Examples (Goodfellow et al.)](https://arxiv.org/abs/1412.6572)
- [Adversarial Robustness Toolbox Documentation](https://adversarial-robustness-toolbox.readthedocs.io/)
