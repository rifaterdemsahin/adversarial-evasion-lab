{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 - Video 1: Fast Gradient Sign Method (FGSM)\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "In this notebook, you will learn:\n",
    "- What is the Fast Gradient Sign Method (FGSM)\n",
    "- How FGSM generates adversarial examples\n",
    "- How to implement FGSM from scratch\n",
    "- How to visualize adversarial perturbations\n",
    "- How adversarial examples fool neural networks\n",
    "\n",
    "## üìö Background\n",
    "\n",
    "The Fast Gradient Sign Method (FGSM) is one of the simplest and most fundamental adversarial attack techniques. It was introduced by Ian Goodfellow et al. in 2014. FGSM creates adversarial examples by adding small perturbations to the input in the direction of the gradient of the loss function.\n",
    "\n",
    "### Mathematical Formula\n",
    "\n",
    "The adversarial example is generated as:\n",
    "\n",
    "$$x_{adv} = x + \\epsilon \\cdot sign(\\nabla_x J(\\theta, x, y))$$\n",
    "\n",
    "Where:\n",
    "- $x$ is the original input\n",
    "- $\\epsilon$ is the perturbation magnitude\n",
    "- $\\nabla_x J(\\theta, x, y)$ is the gradient of the loss with respect to the input\n",
    "- $sign()$ returns the sign of the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Define a Simple Neural Network\n",
    "\n",
    "We'll use a simple Convolutional Neural Network for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"Simple CNN for MNIST classification\"\"\"\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Load Dataset\n",
    "\n",
    "We'll use the MNIST dataset for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load MNIST test dataset\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(test_dataset)} test images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Load Pre-trained Model\n",
    "\n",
    "For this demonstration, we'll initialize a model. In practice, you would load a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = SimpleCNN().to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded and set to evaluation mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öîÔ∏è Implement FGSM Attack\n",
    "\n",
    "Now, let's implement the FGSM attack function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    \"\"\"\n",
    "    Generate adversarial example using FGSM\n",
    "    \n",
    "    Args:\n",
    "        image: Original input image\n",
    "        epsilon: Perturbation magnitude\n",
    "        data_grad: Gradient of loss w.r.t. input\n",
    "    \n",
    "    Returns:\n",
    "        perturbed_image: Adversarial example\n",
    "    \"\"\"\n",
    "    # Get the sign of the gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    \n",
    "    # Create the perturbed image\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    \n",
    "    # Clip to maintain valid pixel range\n",
    "    perturbed_image = torch.clamp(perturbed_image, -3, 3)  # Normalized range\n",
    "    \n",
    "    return perturbed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Generate Adversarial Examples\n",
    "\n",
    "Let's test the attack on a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_attack(model, device, test_loader, epsilon):\n",
    "    \"\"\"\n",
    "    Test FGSM attack on test dataset\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network model\n",
    "        device: Device to run on (CPU/GPU)\n",
    "        test_loader: Test data loader\n",
    "        epsilon: Perturbation magnitude\n",
    "    \n",
    "    Returns:\n",
    "        accuracy: Accuracy on adversarial examples\n",
    "        adv_examples: List of adversarial examples\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "    \n",
    "    # Loop through test set\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Set requires_grad to True for input data\n",
    "        data.requires_grad = True\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1]\n",
    "        \n",
    "        # If initially incorrect, skip\n",
    "        if init_pred.item() != target.item():\n",
    "            continue\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = F.nll_loss(output, target)\n",
    "        \n",
    "        # Zero gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Get gradient\n",
    "        data_grad = data.grad.data\n",
    "        \n",
    "        # Generate adversarial example\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "        \n",
    "        # Re-classify\n",
    "        output = model(perturbed_data)\n",
    "        final_pred = output.max(1, keepdim=True)[1]\n",
    "        \n",
    "        if final_pred.item() == target.item():\n",
    "            correct += 1\n",
    "        else:\n",
    "            # Save some adversarial examples for visualization\n",
    "            if len(adv_examples) < 5:\n",
    "                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                adv_examples.append((init_pred.item(), final_pred.item(), adv_ex))\n",
    "        \n",
    "        # Only test on first 100 examples for speed\n",
    "        if len(adv_examples) >= 5:\n",
    "            break\n",
    "    \n",
    "    # Calculate final accuracy\n",
    "    final_acc = correct / float(len(adv_examples) + correct)\n",
    "    print(f\"Epsilon: {epsilon}\\tTest Accuracy = {correct}/{correct + len(adv_examples)} = {final_acc:.4f}\")\n",
    "    \n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Run Attack with Different Epsilon Values\n",
    "\n",
    "Let's test the attack with different perturbation magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different epsilon values\n",
    "epsilons = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "accuracies = []\n",
    "examples = []\n",
    "\n",
    "# Run test for each epsilon\n",
    "for eps in epsilons:\n",
    "    acc, ex = test_attack(model, device, test_loader, eps)\n",
    "    accuracies.append(acc)\n",
    "    examples.append(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualize Results\n",
    "\n",
    "Let's visualize how accuracy degrades with increasing epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy vs epsilon\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epsilons, accuracies, marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Epsilon (Perturbation Magnitude)', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.title('Accuracy vs Epsilon - FGSM Attack', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim([0, 1.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Visualize Adversarial Examples\n",
    "\n",
    "Let's visualize some adversarial examples to see the perturbations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot several examples at different epsilons\n",
    "cnt = 0\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i in range(len(epsilons)):\n",
    "    for j in range(min(len(examples[i]), 3)):\n",
    "        cnt += 1\n",
    "        plt.subplot(len(epsilons), 3, cnt)\n",
    "        plt.xticks([], [])\n",
    "        plt.yticks([], [])\n",
    "        if j == 0:\n",
    "            plt.ylabel(f\"Eps: {epsilons[i]}\", fontsize=12)\n",
    "        orig, adv, ex = examples[i][j]\n",
    "        plt.title(f\"Orig: {orig} -> Adv: {adv}\")\n",
    "        plt.imshow(ex, cmap=\"gray\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "1. **FGSM is Simple but Effective**: With just one gradient computation, we can generate adversarial examples\n",
    "2. **Epsilon Controls Attack Strength**: Larger epsilon values create stronger perturbations but may be more visible\n",
    "3. **Trade-off Between Success and Perceptibility**: There's a balance between fooling the model and keeping perturbations imperceptible\n",
    "4. **Gradient-Based Attacks**: FGSM demonstrates the vulnerability of neural networks to gradient-based attacks\n",
    "\n",
    "## üîç Next Steps\n",
    "\n",
    "- Try implementing other attack methods (PGD, C&W)\n",
    "- Explore defense mechanisms (adversarial training, input preprocessing)\n",
    "- Test on different models and datasets\n",
    "- Measure perceptibility using metrics like PSNR or SSIM\n",
    "\n",
    "## üìö References\n",
    "\n",
    "1. Goodfellow, I. J., Shlens, J., & Szegedy, C. (2014). Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.\n",
    "2. Madry, A., et al. (2017). Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
